{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f7fc0f-8b9c-4983-981e-e9fc2af66a34",
   "metadata": {},
   "source": [
    "# Embedding Video Caption Text for Search\n",
    "\n",
    "## What are embeddings?\n",
    "\n",
    "OpenAIâ€™s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:\n",
    "\n",
    "- **Search (where results are ranked by relevance to a query string)**\n",
    "- Recommendations (where items with related text strings are recommended)\n",
    "- Clustering (where text strings are grouped by similarity)\n",
    "- Anomaly detection (where outliers with little relatedness are identified)\n",
    "- Diversity measurement (where similarity distributions are analyzed)\n",
    "- Classification (where text strings are classified by their most similar label)\n",
    "\n",
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness.\n",
    "\n",
    "**Small distances suggest high relatedness and large distances suggest low relatedness.**\n",
    "\n",
    "https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da928890-748e-4486-8969-398b80e064ee",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Previously, we proposed integrating lecture-specific information with a student's question to ensure that GPT's responses are contextualized accurately. We can achieve this by creating embeddings from all CS50 video captions, say from 2022, and retrieving relevant information based on these embeddings. The top N results, as determined by the shortest distance to the question, will then be incorporated into our few-shot prompting approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff196e8c-953f-4800-8139-b98d1bf65c64",
   "metadata": {},
   "source": [
    "## SRT Preprocessing\n",
    "\n",
    "The following demonstrates one possible data processing pipeline to create embeddings from raw SRT files.\n",
    "\n",
    "The end result is a JSONL document containing hundreds of thousands of JSON documents. Each document represents a fraction of the lecture caption text, its corresponding embeddings, and useful metadata (e.g., week number, YouTube video ID, etc.). This JSONL document will eventually be loaded into a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58102ceb-f829-4635-af07-420cd9e9f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c7bda-e64a-4ab7-8efd-746f9d734388",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTION_PATH = \"../data/caption_srts/cs50-2023\"\n",
    "SRT_FILES = {\n",
    "    \"lecture0\": { \"video_name\": \"Week 0\", \"youtube_id\": \"3LPJfIKxwWc\" },\n",
    "    \"lecture1\": { \"video_name\": \"Week 1\", \"youtube_id\": \"cwtpLIWylAw\" },\n",
    "    \"lecture2\": { \"video_name\": \"Week 2\", \"youtube_id\": \"4vU4aEFmTSo\" },\n",
    "    \"lecture3\": { \"video_name\": \"Week 3\", \"youtube_id\": \"jZzyERW7h1A\" },\n",
    "    \"lecture4\": { \"video_name\": \"Week 4\", \"youtube_id\": \"F9-yqoS7b8w\" },\n",
    "    \"lecture5\": { \"video_name\": \"Week 5\", \"youtube_id\": \"0euvEdPwQnQ\" },\n",
    "    \"lecture6\": { \"video_name\": \"Week 6\", \"youtube_id\": \"EHi0RDZ31VA\" },\n",
    "    \"lecture7\": { \"video_name\": \"Week 7\", \"youtube_id\": \"1RCMYG8RUSE\" },\n",
    "    \"lecture8\": { \"video_name\": \"Week 8\", \"youtube_id\": \"ciz2UaifaNM\" },\n",
    "    \"lecture9\": { \"video_name\": \"Week 9\", \"youtube_id\": \"-aqUek49iL8\" },\n",
    "    \"ai\": { \"video_name\": \"Artificial Intelligence\", \"youtube_id\": \"6X58aP7yXC4\" },\n",
    "    \"cybersecurity\": { \"video_name\": \"Cybersecurity\", \"youtube_id\": \"EKof-cJiTG8\" },\n",
    "    \"section1\": { \"video_name\": \"Section 1\", \"youtube_id\": \"Tw2-No1J5j0\" },\n",
    "    \"section2\": { \"video_name\": \"Section 2\", \"youtube_id\": \"tnbPMzwSN7A\" },\n",
    "    \"section3\": { \"video_name\": \"Section 3\", \"youtube_id\": \"DdaRHPGhe-E\" },\n",
    "    \"section4\": { \"video_name\": \"Section 4\", \"youtube_id\": \"m2WzPVd4QIc\" },\n",
    "    \"section5\": { \"video_name\": \"Section 5\", \"youtube_id\": \"VqCbWinLqsc\" },\n",
    "    \"section6\": { \"video_name\": \"Section 6\", \"youtube_id\": \"Y07zwrbq4Lc\" },\n",
    "    \"section7\": { \"video_name\": \"Section 7\", \"youtube_id\": \"DQ-OAvbaN4k\" },\n",
    "    \"section8\": { \"video_name\": \"Section 8\", \"youtube_id\": \"DIdm5ubBZIs\" },\n",
    "    \"section9\": { \"video_name\": \"Section 9\", \"youtube_id\": \"IkpaPGqlDHU\" },\n",
    "    \"mario-less\": { \"video_name\": \"Mario (Less Comfortable)\", \"youtube_id\": \"NAs4FIWkJ4s\" },\n",
    "    \"mario-more\": { \"video_name\": \"Mario (More Comfortable)\", \"youtube_id\": \"FzN9RAjYG_Q\" },\n",
    "    \"cash\": { \"video_name\": \"Cash\", \"youtube_id\": \"Y3nWGvqt_Cg\" },\n",
    "    \"credit\": { \"video_name\": \"Credit\", \"youtube_id\": \"dF7wNjsRBjI\" },\n",
    "    \"readability\": { \"video_name\": \"Readability\", \"youtube_id\": \"AOVyZEh9zgE\" },\n",
    "    \"caesar\": { \"video_name\": \"Caesar\", \"youtube_id\": \"V2uusmv2wxI\" },\n",
    "    \"substitution\": { \"video_name\": \"Substitution\", \"youtube_id\": \"cXAoZAsgxJ4\" },\n",
    "    \"plurality\": { \"video_name\": \"Plurality\", \"youtube_id\": \"ftOapzDjEb8\" },\n",
    "    \"runoff\": { \"video_name\": \"Runoff\", \"youtube_id\": \"-Vc5aGywKxo\" },\n",
    "    \"tideman\": { \"video_name\": \"Tideman\", \"youtube_id\": \"kb83NwyYI68\" },\n",
    "    \"filter-less-intro\": { \"video_name\": \"Filter (Less Comfortable)\", \"youtube_id\": \"K0v9byp9jd0\" },\n",
    "    \"filter-more-intro\": { \"video_name\": \"Filter (More Comfortable)\", \"youtube_id\": \"vsOsctDernw\" },\n",
    "    \"filter-less-blur\": { \"video_name\": \"Filter / Blur (Less Comfortable)\", \"youtube_id\": \"6opWB7DaFCY\" },\n",
    "    \"filter-more-blur\": { \"video_name\": \"Filter / Blur (More Comfortable)\", \"youtube_id\": \"dxNO-hCjT0w\" },\n",
    "    \"filter-grayscale\": { \"video_name\": \"Filter / Grayscale\", \"youtube_id\": \"A8LA2osnAwM\" },\n",
    "    \"filter-sepia\": { \"video_name\": \"Filter / Sepia\", \"youtube_id\": \"m0_vouQLufc\" },\n",
    "    \"filter-reflect\": { \"video_name\": \"Filter / Reflect\", \"youtube_id\": \"dlWpx8gQdFo\" },\n",
    "    \"speller\": { \"video_name\": \"Speller\", \"youtube_id\": \"_z57x5PGF4w\" },\n",
    "    \"speller-load\": { \"video_name\": \"Speller / Load\", \"youtube_id\": \"-BX4wLZRwbc\" },\n",
    "    \"speller-hash\": { \"video_name\": \"Speller / Hash\", \"youtube_id\": \"aFe05MQ56Rc\" },\n",
    "    \"speller-size\": { \"video_name\": \"Speller / Size\", \"youtube_id\": \"3cD-_NGTw9A\" },\n",
    "    \"speller-check\": { \"video_name\": \"Speller / Check\", \"youtube_id\": \"qPz_Mr69yE0\" },\n",
    "    \"speller-unload\": { \"video_name\": \"Speller / Unload\", \"youtube_id\": \"qkC4l0pUvCk\" },\n",
    "    \"dna\": { \"video_name\": \"DNA\", \"youtube_id\": \"j84b_EgntcQ\" },\n",
    "    \"movies\": { \"video_name\": \"Movies\", \"youtube_id\": \"v5_A3giDlQs\" },\n",
    "    \"fiftyville\": { \"video_name\": \"Fiftyville\", \"youtube_id\": \"x7Q8tJMi7cQ\" },\n",
    "    \"finance\": { \"video_name\": \"Finance\", \"youtube_id\": \"7wPTAwT-6bA\" }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e04c8c-792c-4deb-9df2-bf07747d42e4",
   "metadata": {},
   "source": [
    "Let's first take a look at what the SRT file looks like.\n",
    "\n",
    "https://www.3playmedia.com/blog/create-srt-file/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8139acb-dad7-4ec6-97bf-8f56267f7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{CAPTION_PATH}/lecture0.srt\") as f:\n",
    "    print(\"\\n\".join(f.read().splitlines()[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb68e8c-33ba-4e2c-a2af-8d631da284c7",
   "metadata": {},
   "source": [
    "**Here are a few things to pay attention:**\n",
    "\n",
    "1. We want to convert all time code to seconds.\n",
    "2. We want to attach metadata for each line of the caption text for later use.\n",
    "3. We would want to remove the speaker name from the caption text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79661f05-4960-4609-84f3-34492f8802fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timecode(timecode):\n",
    "    \"\"\"parse hh:mm:ss,mmm to seconds\"\"\"\n",
    "    timecode = timecode.split(\":\")\n",
    "    timecode = int(timecode[0]) * 3600 + int(timecode[1]) * 60 + float(timecode[2].replace(\",\", \".\"))\n",
    "    return round(timecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cddaf8-6939-40f8-aa79-bbfd55106928",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_document = []\n",
    "for caption in SRT_FILES:\n",
    "\n",
    "    with open(f\"{CAPTION_PATH}/{caption}.srt\") as f:\n",
    "\n",
    "        # remove doule empty lines and split lines based on empty line\n",
    "        lines = f.read()\n",
    "        lines = lines.replace(\"\\n\\n\\n\", \"\\n\\n\")\n",
    "        lines = lines.split(\"\\n\\n\")\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            # remove SRT sequence number and skip empty lines\n",
    "            line = line.split(\"\\n\")[1:]\n",
    "\n",
    "            if (len(line) < 2):\n",
    "                continue\n",
    "\n",
    "            # remove speaker name\n",
    "            if (\"DAVID MALAN:\" in line[1].upper()):\n",
    "                line[1] = line[1].replace(\"DAVID MALAN:\", \"\")\n",
    "\n",
    "            # create JSON document\n",
    "            video_name = SRT_FILES[caption][\"video_name\"]\n",
    "            youtube_id = SRT_FILES[caption][\"youtube_id\"]\n",
    "            start_time = line[0].split(\" --> \")[0]\n",
    "            end_time = line[0].split(\" --> \")[1]\n",
    "\n",
    "            metadata = {}\n",
    "            metadata[\"week\"] = video_name\n",
    "            metadata[\"youtube_id\"] = youtube_id\n",
    "            metadata[\"start\"] = parse_timecode(start_time)\n",
    "            metadata[\"end\"] = parse_timecode(end_time)\n",
    "\n",
    "            json_doc = {}\n",
    "            json_doc[\"text\"] = (\" \").join(line[1:]).strip()\n",
    "            json_doc[\"metadata\"] = metadata\n",
    "\n",
    "            # skip empty caption text\n",
    "            if (json_doc[\"text\"] == \"\"):\n",
    "                continue\n",
    "\n",
    "            # add to JSONL document\n",
    "            jsonl_document.append(json_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6121d8f-589b-4757-ac49-00933d3186b2",
   "metadata": {},
   "source": [
    "**Let's take a look at what this JSONL document looks like:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf5b9d-7290-4d52-bf20-2c1925cf7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total caption text in JSONL document: {len(jsonl_document)}\", end=\"\\n\\n\")\n",
    "\n",
    "preview_lines = 10\n",
    "print(f\"First {preview_lines} line(s) of JSONL document:\")\n",
    "for index, each in enumerate(jsonl_document[:preview_lines]):\n",
    "    print(f\"{index + 1}:\", each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98f32d-9097-40ed-a41d-811d6a56e8a9",
   "metadata": {},
   "source": [
    "**Each line of the JSONL document is a valid JSON file containing lecture caption text, start/end time, YouTube video ID, and week name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd43b6-4fb1-4c3d-afdf-ae45088d565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create processed_data directory if needed\n",
    "if not os.path.exists(\"../data/processed_data\"):\n",
    "    os.makedirs(\"../data/processed_data\")\n",
    "\n",
    "# save to JSONL file\n",
    "with open(\"../data/processed_data/lectures_2023_raw.jsonl\", \"w\") as f:\n",
    "    for chunk in jsonl_document:\n",
    "        f.write(f\"{json.dumps(chunk)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259e7de-fa68-4cce-a8c1-7c0df3c3f26a",
   "metadata": {},
   "source": [
    "## Merge caption texts to span a longer duration\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Caption text serves as a subtitle for videos, typically spanning just a few seconds to ensure readability during video playback. However, this feature doesn't significantly aid in embedding search.\n",
    "\n",
    "Consider the search results in this context; you'd likely find more fragmented pieces of caption text rather than complete sentences. These fragments often lack the necessary contextual information, rendering them not particularly helpful.\n",
    "\n",
    "To enhance search capabilities and offer a more valuable context, we aim to decrease this granularity (e.g., increase the duration for which the caption text appears)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93084dd-d0c2-429a-8ace-fb9f3452971a",
   "metadata": {},
   "source": [
    "**We need to strike a balance in picking the optimal duration so it doesn't result in too many caption texts per JSON document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c299ac-4ab7-44fd-a89d-fbc4916ca1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we merge caption texts to form a new caption text that spans around 30 seconds\n",
    "SPAN_THRESHOLD = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c644d-ea33-4fc4-9298-1554b39cfece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open JSONL file\n",
    "jsonl_document = []\n",
    "with open('../data/processed_data/lectures_2023_raw.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        jsonl_document.append(json.loads(line))\n",
    "\n",
    "# for each json document, merge the subtitles into one string into 30 seconds chunks\n",
    "new_jsonl_document = []\n",
    "total_documents = len(jsonl_document)\n",
    "span = 0\n",
    "running_counter = 0\n",
    "current_pointer = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    # if we are at the end of the file, break\n",
    "    if (current_pointer + running_counter + 1 > total_documents):\n",
    "        break\n",
    "\n",
    "    # calculate the span between the current document and the next document\n",
    "    if (span < 0):\n",
    "        print(\"ERROR: span is less than 0\")\n",
    "        print(\"current pointer: \" + str(current_pointer))\n",
    "        print(\"span: \" + str(span))\n",
    "        print(\"running counter: \" + str(running_counter))\n",
    "        break\n",
    "\n",
    "    # if the next document is in the same week, merge the documents\n",
    "    # if we are at the end of the file, append the last document\n",
    "    running_counter += 1\n",
    "    if (current_pointer + running_counter == total_documents):\n",
    "        new_jsonl_document.append(jsonl_document[current_pointer])\n",
    "        break\n",
    "\n",
    "    if (jsonl_document[current_pointer][\"metadata\"][\"week\"] == jsonl_document[current_pointer + running_counter][\"metadata\"][\"week\"]):\n",
    "        span = jsonl_document[current_pointer + running_counter][\"metadata\"][\"end\"] - jsonl_document[current_pointer][\"metadata\"][\"start\"]\n",
    "        jsonl_document[current_pointer][\"text\"] += \" \" + jsonl_document[current_pointer + running_counter][\"text\"]\n",
    "        jsonl_document[current_pointer][\"metadata\"][\"end\"] = jsonl_document[current_pointer + running_counter][\"metadata\"][\"end\"]\n",
    "\n",
    "        # if the span is greater than the threshold, append the document and reset the pointer\n",
    "        if (span >= SPAN_THRESHOLD):\n",
    "            new_jsonl_document.append(jsonl_document[current_pointer])\n",
    "            span = 0\n",
    "            current_pointer += running_counter + 1\n",
    "            running_counter = 0\n",
    "            continue\n",
    "\n",
    "    # if the next document is not in the same week, append the document and reset the pointer\n",
    "    else:\n",
    "        new_jsonl_document.append(jsonl_document[current_pointer])\n",
    "        current_pointer += running_counter\n",
    "        running_counter = 0\n",
    "        span = 0\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b206158-89e2-4205-8722-18453b5040b2",
   "metadata": {},
   "source": [
    "**Let's look at what the new JSONL document looks like:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48b3c9-1a89-444e-acf4-6688f6795eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total caption text in JSONL document: {len(new_jsonl_document)}\", end=\"\\n\\n\")\n",
    "\n",
    "preview_lines = 5\n",
    "print(f\"First {preview_lines} line(s) of JSONL document:\")\n",
    "for index, each in enumerate(new_jsonl_document[:preview_lines]):\n",
    "    print(f\"{index + 1}:\", each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961ac6a-21e7-406a-afec-e6e015e3909f",
   "metadata": {},
   "source": [
    "**You should see that each JSON document's caption text should be longer, spanning a longer duration. This would give us a more useful and relevant lecture context in the prompt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8367f5-766f-47f7-b2a6-ecba3db55067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new JSONL file\n",
    "with open('../data/processed_data/lectures_2023_merged.jsonl', 'w') as f:\n",
    "    f.write('\\n'.join(json.dumps(i) for i in new_jsonl_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f67b8-22cd-4375-ac3d-f3ef336a49c1",
   "metadata": {},
   "source": [
    "## Create embeddings for caption texts\n",
    "\n",
    "We are now ready to create embeddings for caption texts.\n",
    "\n",
    "**Note that we only generate embeddings for each caption text, we do not generate embeddings for metadata.**\n",
    "\n",
    "Reference: https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b913f9-431b-4f37-b865-29d675d5f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_document = []\n",
    "text_documents = [] # easier to work with OpenAI's API\n",
    "\n",
    "with open(\"../data/processed_data/lectures_2023_merged.jsonl\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = json.loads(line)\n",
    "        jsonl_document.append(line)\n",
    "        text_documents.append(line[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d6268-2201-4a46-b1c6-cdabaf5be325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jsonl_document[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf89f6-e08f-4534-bdca-e9b6bab942d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBEDDING_TOKEN_LIMIT = 8191\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "def num_tokens_from_string(string: str, model_name: str = \"gpt-4\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    # https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total tokens needed and cost\n",
    "# https://openai.com/pricing#language-models\n",
    "total_tokens = num_tokens_from_string(\"\".join(text_documents))\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"Total embedding cost: ${round(total_tokens * 0.1 / 1000000, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c08267-5738-40a3-a4c8-605f7ceb77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "embeddings = []\n",
    "\n",
    "try:\n",
    "    \n",
    "    # this is a rather expensive operation, proceed with caution \n",
    "    if (input(\"Proceed to create embeddings? (y/n) \") != \"y\"):\n",
    "        raise KeyboardInterrupt\n",
    "\n",
    "    for batch_start in range(0, len(text_documents), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        batch = text_documents[batch_start:batch_end]\n",
    "        print(f\"Batch {batch_start} to {batch_end-1} of {len(text_documents)}\")\n",
    "\n",
    "        # call OpenAI API to create embedding for a given caption text\n",
    "        response = openai.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "\n",
    "        # double check embeddings are in same order as input\n",
    "        for i, be in enumerate(response.data):\n",
    "            assert i == be.index\n",
    "\n",
    "        batch_embeddings = [e.embedding for e in response.data]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    print(\"Finished creating embeddings\")\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"operation aborted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c02b6-d8e9-4a7c-aadf-83a8dc3995b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save jsonl file with embeddings\n",
    "with open(\"../data/processed_data/lectures_2023_embeddings.jsonl\", \"w\") as f:\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "\n",
    "        # store embeddings for a given caption text\n",
    "        jsonl_document[i][\"embedding\"] = embedding\n",
    "        f.write(json.dumps(jsonl_document[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6069e-15b4-4118-86f6-a45e495e6c29",
   "metadata": {},
   "source": [
    "\n",
    "**You can take a look at what the embedding looks like for each caption text.**\n",
    "\n",
    "**Note that because we use the `text-embedding-ada-002` model, we will always get a 1536-dimensional embedding vector (i.e., there are 1536 numbers inside).**\n",
    "\n",
    "Reference: https://platform.openai.com/docs/guides/embeddings/second-generation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a538eb-dedb-4342-9b74-930f200d7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total caption text in JSONL document: {len(jsonl_document)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d91015-9b3c-428e-a921-218d64b866e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_line = 42\n",
    "json_doc = jsonl_document[preview_line-1]\n",
    "\n",
    "print(f\"Total characters: {len(json_doc['text'])}\", f\"embedding size: {len(json_doc['embedding'])}\\n\")\n",
    "print(f\"plain_text:\\n{json_doc['text']}\\n\")\n",
    "print(f\"embedding:\\n{json_doc['embedding']}\\n\")\n",
    "print(f\"metadata:\\n{json_doc['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac2716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
