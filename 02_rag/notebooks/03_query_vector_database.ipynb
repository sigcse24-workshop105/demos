{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f84825da-4e8c-484a-9325-1c108a52ce9d",
   "metadata": {},
   "source": [
    "# Question-answering\n",
    "\n",
    "Now that we have our vector database created and all our embeddings for caption text had been stored properly. We can now search for relevant caption text using student's question.\n",
    "\n",
    "Reference: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14adabeb-3b03-4153-9c5e-1b7bbf340c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import chromadb\n",
    "import tiktoken\n",
    "import time\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from termcolor import colored"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d383bdc-212b-415d-b606-043c51614308",
   "metadata": {},
   "source": [
    "**Because the embeddings for our caption text are created by OpenAI, naturally, we should choose to use OpenAI's embedding function to create embedding for student's question as well.**\n",
    "\n",
    "**Strictly speaking, we have to use OpenAI's embedding function to ensure the output dimension is aligned with our caption text embeddings (e.g., 1536).**\n",
    "\n",
    "The following codes connect to a chroma db we created earlier and get a reference to our `cs50-lectures-2022` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd5e9f-2075-461a-8127-8096b082a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-4\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# use openai embedding function\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# setup chroma db client\n",
    "client = chromadb.PersistentClient(path=\"../vector_db\")\n",
    "\n",
    "collection = client.get_collection(\"cs50_lectures_2022\", embedding_function=openai_ef)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83dd5cbc-36df-4978-a819-4eaede7cdfb3",
   "metadata": {},
   "source": [
    "Some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235bbd5-b790-44b9-bb79-db0df7c57a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def print_cost(message):\n",
    "    print(f\"Total tokens used: {num_tokens(message, GPT_MODEL)}, cost: {round(num_tokens(message, GPT_MODEL) * 0.002 / 1000, 6)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99aa090a-970d-4073-9fa6-9371c9f19f7a",
   "metadata": {},
   "source": [
    "## Construct our Prompt\n",
    "\n",
    "With embeddings, we can now construct our prompt with caption texts that are matched with student's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2647b-ea69-4684-9e46-a59b278c7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_message(query, collection, token_budget=4096 - 500, print_result=False):\n",
    "\n",
    "    # query vector database and return the top N closest matchest\n",
    "    # https://docs.trychroma.com/usage-guide#querying-a-collection\n",
    "    results = collection.query(\n",
    "        query_texts=[query.strip()],\n",
    "        n_results=2,\n",
    "    )\n",
    "\n",
    "    # an introductory text for our prompt\n",
    "    introduction = \"\"\"Use the transcripts below from the CS50 lectures \\\n",
    "taught by David Malan as useful resources to answer questions. Make \\\n",
    "sure your answer is accurate. Don't offer solution to the question, \\\n",
    "only offer helpful hints. Mention in which week the concept is taught \\\n",
    "and provide relevant time codes if necessary.\"\"\"\n",
    "    \n",
    "    # build our message string progressively\n",
    "    message = introduction\n",
    "\n",
    "    # prompt tuning for the best results\n",
    "    additional_instructions = \"\"\" \\\n",
    "You should not answer questions that are not related to the course material. \\\n",
    "When answering questions, use correct terminology, grammar, and punctuation. \\\n",
    "If you need to address the student in your answers, always use second person. \\\n",
    "Remember that students may have incorrect assumptions about the course material. \\\n",
    "If you are unsure about the answer, you can say that you do not know.\"\"\"\n",
    "    message += additional_instructions\n",
    "\n",
    "    # our student's question, we will include it at the end to our prompt\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "\n",
    "    # print result for debugging\n",
    "    if print_result:\n",
    "        print(colored(\"Embeddings search result:\", \"red\"))\n",
    "\n",
    "    # we can also present user with YouTube playback URls\n",
    "    # for the relevant caption text, so they can easily\n",
    "    # jump to a lecture video and rewatch key concepts\n",
    "    references = []\n",
    "    \n",
    "    for index, distance in enumerate(results[\"distances\"][0]):\n",
    "\n",
    "        # our caption text\n",
    "        caption_text = results[\"documents\"][0][index].strip()\n",
    "\n",
    "        # some useful metadata we can also include in the prompt\n",
    "        week_number = results[\"metadatas\"][0][index][\"week\"]\n",
    "        youtude_id = results[\"metadatas\"][0][index][\"youtube_id\"]\n",
    "        start_time = results[\"metadatas\"][0][index][\"start\"]\n",
    "\n",
    "        # generate a playback URL for users (not added to the prompt)\n",
    "        playback_url = f\"https://www.youtube.com/watch?v={youtude_id}&t={start_time}s\"\n",
    "        references.append(playback_url)\n",
    "\n",
    "        # convert secods to hh:mm:ss format to improve readability\n",
    "        m, s = divmod(start_time, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        timecode = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "        if print_result:\n",
    "            print(colored(f\"document {index+1}:\", \"red\"))\n",
    "            print(colored(f\"distance score: {distance}\", \"red\"))\n",
    "            print(colored(results[\"documents\"][0][index], \"red\"))\n",
    "            print(colored(results[\"metadatas\"][0][index], \"red\"))\n",
    "            print(colored(\"=\" * 10, \"red\"))\n",
    "\n",
    "        next_transcript = f'\\n\\nMentioned at: {timecode} in {week_number}:\\n\"\"\"\\n{caption_text}\\n\"\"\"\\n'\n",
    "        running_token_count = num_tokens(message + next_transcript + question, GPT_MODEL)\n",
    "\n",
    "        # ensure our prompt doesn't exceed our budget as well as model token limit\n",
    "        if (running_token_count> token_budget):\n",
    "            break\n",
    "        else:\n",
    "            message += next_transcript\n",
    "\n",
    "    # append student's question at the end\n",
    "    message += question\n",
    "\n",
    "    return message, references"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919029a6-1f0d-45e2-84f7-45254a989b95",
   "metadata": {},
   "source": [
    "## Ask\n",
    "\n",
    "Supply GPT's chat completion endpoint with our prompt and obtain a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cedd4e-a6b2-4643-80ee-b8a6f4be9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query_message, print_message=False):\n",
    "\n",
    "    if print_message:\n",
    "        print(colored(\"=== Begin prompt ===\", \"blue\"))\n",
    "        print(colored(query_message, \"green\"))\n",
    "        print(colored(\"=== End prompt ===\", \"blue\"))\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a computer science professor.\"},\n",
    "        {\"role\": \"user\", \"content\": query_message},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fb8a140",
   "metadata": {},
   "source": [
    "## Check for Academic Honesty\n",
    "\n",
    "Use a second API call to ask GPT to revise its own response, in line with an abridged version of the course's academic honesty policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_academic_honesty(bot_response, print_message=False):\n",
    "    instructions = \"\"\"You are reviewing content that may or may not violate the course's academic honesty policy. \\\n",
    "Remove any parts of the content that mention: sharing/posting code, looking for solutions online, or asking classmates for help. \\\n",
    "Do not change any other parts of the content. Once you are done reviewing the content, reply only with the content.\"\"\"\n",
    "\n",
    "    prompt = instructions + f\"\\n\\nContent: {bot_response}\"\n",
    "    \n",
    "    if print_message:\n",
    "        print(colored(\"=== Begin check prompt ===\", \"magenta\"))\n",
    "        print(colored(prompt, \"cyan\"))\n",
    "        print(colored(\"=== End check prompt ===\", \"magenta\"))\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are reviewing content to ensure compliance with the course's academic honesty policy.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return f\"==> 💾 GPT: {response_message}\\n\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad2e5dcf-67ab-44af-9bd7-876f6b517148",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "The following describes a single question-answering session:\n",
    "\n",
    "1. The student asks a question.\n",
    "2. We perform an embeddings search using the student's question and obtain relevant information, such as caption texts.\n",
    "3. We construct a prompt with the desired instructions, including the relevant caption texts.\n",
    "4. We query GPT's chat completion endpoint.\n",
    "5. We ask GPT to revise its response, based on academic honesty.\n",
    "6. We render GPT's response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bfea1-1dc4-45ba-9329-7d0ddbb580d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        query = input(\"==> 🧑‍🎓 Student: \")\n",
    "    \n",
    "        start_time = time.time()\n",
    "        query_message, references = generate_query_message(query, collection, print_result=True)\n",
    "        print(f\"Search time took {round(time.time() - start_time, 2)} seconds.\")\n",
    "    \n",
    "        start_time = time.time()\n",
    "        response_message = ask(query_message, print_message=True)\n",
    "\n",
    "        edited_response_message = check_academic_honesty(response_message, print_message=True)\n",
    "        edited_response_message += \"\\nHere are the relevant lecture videos:\\n\" + \"\\n\".join(references)\n",
    "    \n",
    "        print(edited_response_message)\n",
    "        print(f\"Response time took {round(time.time() - start_time, 2)} seconds.\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"stopped\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2560c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
