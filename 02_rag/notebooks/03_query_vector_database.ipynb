{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f84825da-4e8c-484a-9325-1c108a52ce9d",
   "metadata": {},
   "source": [
    "# Question-answering\n",
    "\n",
    "Now that we have our vector database created and all our embeddings for caption text had been stored properly. We can now search for relevant caption text using student's question.\n",
    "\n",
    "Reference: https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db8c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14adabeb-3b03-4153-9c5e-1b7bbf340c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import chromadb\n",
    "import tiktoken\n",
    "import time\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from termcolor import colored"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d383bdc-212b-415d-b606-043c51614308",
   "metadata": {},
   "source": [
    "**Because the embeddings for our caption text are created by OpenAI, naturally, we should choose to use OpenAI's embedding function to create embedding for student's question as well.**\n",
    "\n",
    "**Strictly speaking, we have to use OpenAI's embedding function to ensure the output dimension is aligned with our caption text embeddings (e.g., 1536).**\n",
    "\n",
    "The following codes connect to a chroma db we created earlier and get a reference to our `cs50-lectures-2022` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9edd5e9f-2075-461a-8127-8096b082a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-4\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# use openai embedding function\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# setup chroma db client\n",
    "client = chromadb.PersistentClient(path=\"../vector_db\")\n",
    "\n",
    "collection = client.get_collection(\"cs50_lectures_2022\", embedding_function=openai_ef)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83dd5cbc-36df-4978-a819-4eaede7cdfb3",
   "metadata": {},
   "source": [
    "Some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9235bbd5-b790-44b9-bb79-db0df7c57a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def print_cost(message):\n",
    "    print(f\"Total tokens used: {num_tokens(message, GPT_MODEL)}, cost: {round(num_tokens(message, GPT_MODEL) * 0.002 / 1000, 6)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99aa090a-970d-4073-9fa6-9371c9f19f7a",
   "metadata": {},
   "source": [
    "## Construct our Prompt\n",
    "\n",
    "With embeddings, we can now construct our prompt with caption texts that are matched with student's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b2647b-ea69-4684-9e46-a59b278c7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_message(query, collection, token_budget=4096 - 500, print_result=False):\n",
    "\n",
    "    # query vector database and return the top N closest matchest\n",
    "    # https://docs.trychroma.com/usage-guide#querying-a-collection\n",
    "    results = collection.query(\n",
    "        query_texts=[query.strip()],\n",
    "        n_results=2,\n",
    "    )\n",
    "\n",
    "    # an introductory text for our prompt\n",
    "    introduction = \"\"\"Use the transcripts below from the CS50 lectures \\\n",
    "taught by David Malan as useful resources to answer questions. Make \\\n",
    "sure your answer is accurate. Don't offer solution to the question, \\\n",
    "only offer helpful hints. Mention in which week the concept is taught \\\n",
    "and provide relevant time codes if necessary.\"\"\"\n",
    "    \n",
    "    # build our message string progressively\n",
    "    message = introduction\n",
    "\n",
    "    # prompt tuning for the best results\n",
    "    additional_instructions = \"\"\" \\\n",
    "You should not answer questions that are not related to the course material. \\\n",
    "When answering questions, use correct terminology, grammar, and punctuation. \\\n",
    "If you need to address the student in your answers, always use second person. \\\n",
    "Remember that students may have incorrect assumptions about the course material. \\\n",
    "If you are unsure about the answer, you can say that you do not know.\"\"\"\n",
    "    message += additional_instructions\n",
    "\n",
    "    # our student's question, we will include it at the end to our prompt\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "\n",
    "    # print result for debugging\n",
    "    if print_result:\n",
    "        print(colored(\"Embeddings search result:\", \"red\"))\n",
    "\n",
    "    # we can also present user with YouTube playback URls\n",
    "    # for the relevant caption text, so they can easily\n",
    "    # jump to a lecture video and rewatch key concepts\n",
    "    references = []\n",
    "    \n",
    "    for index, distance in enumerate(results[\"distances\"][0]):\n",
    "\n",
    "        # our caption text\n",
    "        caption_text = results[\"documents\"][0][index].strip()\n",
    "\n",
    "        # some useful metadata we can also include in the prompt\n",
    "        week_number = results[\"metadatas\"][0][index][\"week\"]\n",
    "        youtude_id = results[\"metadatas\"][0][index][\"youtube_id\"]\n",
    "        start_time = results[\"metadatas\"][0][index][\"start\"]\n",
    "\n",
    "        # generate a playback URL for users (not added to the prompt)\n",
    "        playback_url = f\"https://www.youtube.com/watch?v={youtude_id}&t={start_time}s\"\n",
    "        references.append(playback_url)\n",
    "\n",
    "        # convert secods to hh:mm:ss format to improve readability\n",
    "        m, s = divmod(start_time, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        timecode = f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "        if print_result:\n",
    "            print(colored(f\"document {index+1}:\", \"red\"))\n",
    "            print(colored(f\"distance score: {distance}\", \"red\"))\n",
    "            print(colored(results[\"documents\"][0][index], \"red\"))\n",
    "            print(colored(results[\"metadatas\"][0][index], \"red\"))\n",
    "            print(colored(\"=\" * 10, \"red\"))\n",
    "\n",
    "        next_transcript = f'\\n\\nMentioned at: {timecode} in {week_number}:\\n\"\"\"\\n{caption_text}\\n\"\"\"\\n'\n",
    "        running_token_count = num_tokens(message + next_transcript + question, GPT_MODEL)\n",
    "\n",
    "        # ensure our prompt doesn't exceed our budget as well as model token limit\n",
    "        if (running_token_count> token_budget):\n",
    "            break\n",
    "        else:\n",
    "            message += next_transcript\n",
    "\n",
    "    # append student's question at the end\n",
    "    message += question\n",
    "\n",
    "    return message, references"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919029a6-1f0d-45e2-84f7-45254a989b95",
   "metadata": {},
   "source": [
    "## Ask\n",
    "\n",
    "Supply GPT's chat completion endpoint with our prompt and obtain a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99cedd4e-a6b2-4643-80ee-b8a6f4be9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query_message, print_message=False):\n",
    "\n",
    "    if print_message:\n",
    "        print(colored(\"=== Begin prompt ===\", \"blue\"))\n",
    "        print(colored(query_message, \"green\"))\n",
    "        print(colored(\"=== End prompt ===\", \"blue\"))\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a computer science professor.\"},\n",
    "        {\"role\": \"user\", \"content\": query_message},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fb8a140",
   "metadata": {},
   "source": [
    "## Check for Academic Honesty\n",
    "\n",
    "Use a second API call to ask GPT to revise its own response, in line with an abridged version of the course's academic honesty policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a9608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_academic_honesty(bot_response, print_message=False):\n",
    "    instructions = \"\"\"You are reviewing content that may or may not violate the course's academic honesty policy. \\\n",
    "Remove any parts of the content that mention: sharing/posting code, looking for solutions online, or asking classmates for help. \\\n",
    "Do not change any other parts of the content. Once you are done reviewing the content, reply only with the content.\"\"\"\n",
    "\n",
    "    prompt = instructions + f\"\\n\\nContent: {bot_response}\"\n",
    "    \n",
    "    if print_message:\n",
    "        print(colored(\"=== Begin check prompt ===\", \"magenta\"))\n",
    "        print(colored(prompt, \"cyan\"))\n",
    "        print(colored(\"=== End check prompt ===\", \"magenta\"))\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are reviewing content to ensure compliance with the course's academic honesty policy.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return f\"==> 💾 GPT: {response_message}\\n\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad2e5dcf-67ab-44af-9bd7-876f6b517148",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "The following describes a single question-answering session:\n",
    "\n",
    "1. The student asks a question.\n",
    "2. We perform an embeddings search using the student's question and obtain relevant information, such as caption texts.\n",
    "3. We construct a prompt with the desired instructions, including the relevant caption texts.\n",
    "4. We query GPT's chat completion endpoint.\n",
    "5. We ask GPT to revise its response, based on academic honesty.\n",
    "6. We render GPT's response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a22bfea1-1dc4-45ba-9329-7d0ddbb580d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEmbeddings search result:\u001b[0m\n",
      "\u001b[31mdocument 1:\u001b[0m\n",
      "\u001b[31mdistance score: 0.34150901436805725\u001b[0m\n",
      "\u001b[31msome of our own logic. We saw it with SQL, we're going to now see it with HTML, CSS, and even JavaScript if we want. And we're also going to see another language today, not a programming language, called Jinja. And this is going to be a common paradigm in the real world, whereby different languages, different libraries, different frameworks often borrow from each other, or they use technologies that someone else wrote just so they don't have to reinvent that wheel. So Flask is just a framework. That is a third party library, it's pretty popular nowadays, it's relatively simple, which is why we use it in CS50.\u001b[0m\n",
      "\u001b[31m{'end': 483, 'start': 453, 'week': 'Week 9', 'youtube_id': 'oVA0fD13NGI'}\u001b[0m\n",
      "\u001b[31m==========\u001b[0m\n",
      "\u001b[31mdocument 2:\u001b[0m\n",
      "\u001b[31mdistance score: 0.3606009781360626\u001b[0m\n",
      "\u001b[31mMaybe if you've worked on the homepage problem-- you've been doing a lot of copying and pasting, or you weren't able to do everything you wanted to do. Well, Flask comes in to help you build something that is going to be more dynamic for you. And one of my favorite examples of this, of a more dynamic website that Flask can enable us to build is something like this one right here, libraryofbabel.info. And you can actually go to this site. This site is based on the book, Library of Babel by Jorge Luis Borges. And in this book, this author imagines this library\u001b[0m\n",
      "\u001b[31m{'end': 126, 'start': 95, 'week': 'Section 9', 'youtube_id': 'RmcIhrBN0m0'}\u001b[0m\n",
      "\u001b[31m==========\u001b[0m\n",
      "Search time took 0.59 seconds.\n",
      "\u001b[34m=== Begin prompt ===\u001b[0m\n",
      "\u001b[32mUse the transcripts below from the CS50 lectures taught by David Malan as useful resources to answer questions. Make sure your answer is accurate. Don't offer solution to the question, only offer helpful hints. Mention in which week the concept is taught and provide relevant time codes if necessary. You should not answer questions that are not related to the course material. When answering questions, use correct terminology, grammar, and punctuation. If you need to address the student in your answers, always use second person. Remember that students may have incorrect assumptions about the course material. If you are unsure about the answer, you can say that you do not know.\n",
      "\n",
      "Mentioned at: 00:07:33 in Week 9:\n",
      "\"\"\"\n",
      "some of our own logic. We saw it with SQL, we're going to now see it with HTML, CSS, and even JavaScript if we want. And we're also going to see another language today, not a programming language, called Jinja. And this is going to be a common paradigm in the real world, whereby different languages, different libraries, different frameworks often borrow from each other, or they use technologies that someone else wrote just so they don't have to reinvent that wheel. So Flask is just a framework. That is a third party library, it's pretty popular nowadays, it's relatively simple, which is why we use it in CS50.\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "Mentioned at: 00:01:35 in Section 9:\n",
      "\"\"\"\n",
      "Maybe if you've worked on the homepage problem-- you've been doing a lot of copying and pasting, or you weren't able to do everything you wanted to do. Well, Flask comes in to help you build something that is going to be more dynamic for you. And one of my favorite examples of this, of a more dynamic website that Flask can enable us to build is something like this one right here, libraryofbabel.info. And you can actually go to this site. This site is based on the book, Library of Babel by Jorge Luis Borges. And in this book, this author imagines this library\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "Question: What is flask?\u001b[0m\n",
      "\u001b[34m=== End prompt ===\u001b[0m\n",
      "\u001b[35m=== Begin check prompt ===\u001b[0m\n",
      "\u001b[36mYou are reviewing content that may or may not violate the course's academic honesty policy. Remove any parts of the content that mention: sharing/posting code, looking for solutions online, or asking classmates for help. Do not change any other parts of the content. Once you are done reviewing the content, reply only with the content.\n",
      "\n",
      "Content: Flask is a popular third-party library and framework used in web development. It is designed to help you build dynamic websites, as opposed to static ones. Flask allows you to incorporate your own logic and use different languages, libraries, and frameworks together. This concept is introduced in Week 9 of the CS50 course at the time code 00:07:33.\u001b[0m\n",
      "\u001b[35m=== End check prompt ===\u001b[0m\n",
      "==> 💾 GPT: Flask is a popular third-party library and framework used in web development. It is designed to help you build dynamic websites, as opposed to static ones. Flask allows you to incorporate your own logic and use different languages, libraries, and frameworks together. This concept is introduced in Week 9 of the CS50 course at the time code 00:07:33.\n",
      "\n",
      "Here are the relevant lecture videos:\n",
      "https://www.youtube.com/watch?v=oVA0fD13NGI&t=453s\n",
      "https://www.youtube.com/watch?v=RmcIhrBN0m0&t=95s\n",
      "Response time took 6.82 seconds.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> 🧑‍🎓 Student: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m query_message, references \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_query_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch time took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mgenerate_query_message\u001b[0;34m(query, collection, token_budget, print_result)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_query_message\u001b[39m(query, collection, token_budget\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m500\u001b[39m, print_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# query vector database and return the top N closest matchest\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# https://docs.trychroma.com/usage-guide#querying-a-collection\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# an introductory text for our prompt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     introduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mUse the transcripts below from the CS50 lectures \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mtaught by David Malan as useful resources to answer questions. Make \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124msure your answer is accurate. Don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt offer solution to the question, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124monly offer helpful hints. Mention in which week the concept is taught \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mand provide relevant time codes if necessary.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/chromadb/api/models/Collection.py:327\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_query_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_texts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_query_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m         valid_query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mvalid_query_images)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/chromadb/api/models/Collection.py:633\u001b[0m, in \u001b[0;36mCollection._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m     )\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/chromadb/api/types.py:193\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m--> 193\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(maybe_cast_one_to_many_embedding(result))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:188\u001b[0m, in \u001b[0;36mOpenAIEmbeddingFunction.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Call the OpenAI Embedding API\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v1:\n\u001b[0;32m--> 188\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deployment_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_name\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Sort resulting embeddings by index\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     sorted_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(embeddings, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e: e\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/resources/embeddings.py:113\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/openai/_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    996\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        query = input(\"==> 🧑‍🎓 Student: \")\n",
    "    \n",
    "        start_time = time.time()\n",
    "        query_message, references = generate_query_message(query, collection, print_result=True)\n",
    "        print(f\"Search time took {round(time.time() - start_time, 2)} seconds.\")\n",
    "    \n",
    "        start_time = time.time()\n",
    "        response_message = ask(query_message, print_message=True)\n",
    "\n",
    "        edited_response_message = check_academic_honesty(response_message, print_message=True)\n",
    "        edited_response_message += \"\\nHere are the relevant lecture videos:\\n\" + \"\\n\".join(references)\n",
    "    \n",
    "        print(edited_response_message)\n",
    "        print(f\"Response time took {round(time.time() - start_time, 2)} seconds.\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"stopped\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2560c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
